## 简介 ##
网络爬虫通过下载种子地址中的网页内容，解析里面的链接地址，然后下载对应的内容，再解析，再下载……如此反复爬取互联网上的数据。

```
                      +------+
                      | link |
                      +------+
                         |
                         V
                    +-------------+
                    |     Page    |
                    +------+------+
                    | link | link |
                    +------+------+
                       |       |
                       |       +-------+
                       |               |
                       V               V
                +-------------+  +-----------+
                |     Page    |  |    Page   |
                +------+------+  +-----------+
                | link | link |  |    link   |
                +------+------+  +-----------+
                   |       |           |
           +-------+       |           +--------+
           |               |                    |
           V               V                    V
    +-------------+  +-------------+  +--------------------+
    |     Page    |  |     Page    |  |        Page        |
    +------+------+  +------+------+  +------+------+------+
    | link | link |  | link | link |  | link | link | link |
    +------+------+  +------+------+  +------+------+------+
        |     |         |      |          |      |      |
        |     .         .      .          .      .      .
        V
    +-------------+
    |     Page    |
    +------+------+
    | link | link |
    +------+------+
        |      |
        .      .
```

这是一个“鸡生蛋，蛋生鸡”的过程，会无限制的爬取下去。
为了避免这样的结果，需要过滤处理解析得到的地址，防止过度爬取。

最后的流程：

```
          +--------+
          | start  |
          +--------+
              |
              v
       +--------------+
       |     url      |
       +--------------+
              | <-------------------------------+
              v                                 |
       +--------------+                         |
       |   download   |                         |
       +--------------+                         |
              |                                 |
              v                                 |
         +---------+                            |
         | content |                            |
         +---------+                            |
          |       |                             |
          |       +----------------+            |
          |                        |            |
          v                        v            |
    +---------------------+  +---------------+  |
    | extract_interesting |  | extract_urls  |  |
    +---------------------+  +---------------+  |
                                     |          |
                                     v          |
                                 +--------+     |
                                 | filter |     |
                                 +--------+     |
                                     |          |
                                  N  |  Y       |
                  +------------------+----------+
                  |
                  v
              +------+
              | stop |
              +------+
```

大致的代码逻辑：

```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#

def crawler(url):
    content = download(url)
    urls = extract_urls(content)
    something = extract_something(content)
    follow_urls = url_filter(urls)
    for url in follow_urls:
        crawler(url)
```

其中 download 函数通过链接地址下载网页内容， extract_urls 从网页内容中解析链接地址， extract_something 从网页中解析感兴趣的内容， url_filter 过滤解析得到的链接地址，控制爬取范围。
实现这些函数就可以实现一个简单的网络爬虫了。

download 函数将涉及 HTTP 网络通信，extract_urls 和 extract_something 涉及简单的 HTML 文本处理，这些都会在接下来的内容中慢慢的讲。
